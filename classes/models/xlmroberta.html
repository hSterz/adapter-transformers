

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>XLM-RoBERTa &mdash; adapter-transformers  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.png"/>
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="RoBERTa" href="roberta.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> adapter-transformers
          

          
            
            <img src="../../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">adapter-transformers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../adapters.html">Introduction to Adapters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../adapter_composition.html">Adapter Activation and Composition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training.html">Adapter Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../prediction_heads.html">Prediction Heads</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../extending.html">Extending the Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../v2_transition.html">Transitioning from v1 to v2</a></li>
</ul>
<p class="caption"><span class="caption-text">Adapter-Hub</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../loading.html">Loading Pre-Trained Adapters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing to Adapter Hub</a></li>
</ul>
<p class="caption"><span class="caption-text">Adapter-Related Classes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../adapter_config.html">Adapter Configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_adapters_config.html">Model Adapters Config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adapter_modules.html">Adapter Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adapter_layer.html">AdapterLayerBaseMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_mixins.html">Model Mixins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adapter_utils.html">Adapter Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../weights_loaders.html">Weights Loaders</a></li>
</ul>
<p class="caption"><span class="caption-text">Supported Models</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="bart.html">BART</a></li>
<li class="toctree-l1"><a class="reference internal" href="bert.html">BERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="distilbert.html">DistilBERT</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpt2.html">OpenAI GPT2</a></li>
<li class="toctree-l1"><a class="reference internal" href="mbart.html">MBart</a></li>
<li class="toctree-l1"><a class="reference internal" href="roberta.html">RoBERTa</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">XLM-RoBERTa</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#xlmrobertaconfig">XLMRobertaConfig</a></li>
<li class="toctree-l2"><a class="reference internal" href="#xlmrobertatokenizer">XLMRobertaTokenizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#xlmrobertamodel">XLMRobertaModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#xlmrobertamodelwithheads">XLMRobertaModelWithHeads</a></li>
<li class="toctree-l2"><a class="reference internal" href="#xlmrobertaformaskedlm">XLMRobertaForMaskedLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#xlmrobertaforsequenceclassification">XLMRobertaForSequenceClassification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#xlmrobertaformultiplechoice">XLMRobertaForMultipleChoice</a></li>
<li class="toctree-l2"><a class="reference internal" href="#xlmrobertafortokenclassification">XLMRobertaForTokenClassification</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">adapter-transformers</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>XLM-RoBERTa</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/classes/models/xlmroberta.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="xlm-roberta">
<h1>XLM-RoBERTa<a class="headerlink" href="#xlm-roberta" title="Permalink to this headline">¶</a></h1>
<p>The XLM-RoBERTa model was proposed in <a class="reference external" href="https://arxiv.org/abs/1911.02116">Unsupervised Cross-lingual Representation Learning at Scale</a>
by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán,
Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based on Facebook’s RoBERTa model released in 2019.
It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This class is nearly identical to the PyTorch implementation of XLM-RoBERTa in Huggingface Transformers.
For more information, visit <a class="reference external" href="https://huggingface.co/transformers/model_doc/xlmroberta.html">the corresponding section in their documentation</a>.</p>
</div>
<div class="section" id="xlmrobertaconfig">
<h2>XLMRobertaConfig<a class="headerlink" href="#xlmrobertaconfig" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.XLMRobertaConfig">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">XLMRobertaConfig</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">pad_token_id</span><span class="o">=</span><span class="default_value">1</span></em>, <em class="sig-param"><span class="n">bos_token_id</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">eos_token_id</span><span class="o">=</span><span class="default_value">2</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.XLMRobertaConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>This class overrides <a class="reference internal" href="roberta.html#transformers.RobertaConfig" title="transformers.RobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code></a>. Please check the superclass for the appropriate
documentation alongside usage examples.</p>
</dd></dl>

</div>
<div class="section" id="xlmrobertatokenizer">
<h2>XLMRobertaTokenizer<a class="headerlink" href="#xlmrobertatokenizer" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.XLMRobertaTokenizer">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">XLMRobertaTokenizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.XLMRobertaTokenizer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="xlmrobertamodel">
<h2>XLMRobertaModel<a class="headerlink" href="#xlmrobertamodel" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.XLMRobertaModel">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">XLMRobertaModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em>, <em class="sig-param"><span class="n">add_pooling_layer</span><span class="o">=</span><span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.XLMRobertaModel" title="Permalink to this definition">¶</a></dt>
<dd><p>The bare XLM-RoBERTa Model transformer outputting raw hidden-states without any specific head on top.</p>
<p>This model inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a>) – Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method to load the model
weights.</p>
</dd>
</dl>
<p>This class overrides <a class="reference internal" href="roberta.html#transformers.RobertaModel" title="transformers.RobertaModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaModel</span></code></a>. Please check the superclass for the appropriate
documentation alongside usage examples.</p>
<dl class="py attribute">
<dt id="transformers.XLMRobertaModel.config_class">
<code class="sig-name descname">config_class</code><a class="headerlink" href="#transformers.XLMRobertaModel.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.xlm_roberta.configuration_xlm_roberta.XLMRobertaConfig</span></code></p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="xlmrobertamodelwithheads">
<h2>XLMRobertaModelWithHeads<a class="headerlink" href="#xlmrobertamodelwithheads" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.XLMRobertaModelWithHeads">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">XLMRobertaModelWithHeads</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.XLMRobertaModelWithHeads" title="Permalink to this definition">¶</a></dt>
<dd><p>XLM-RoBERTa Model with the option to add multiple flexible heads on top.</p>
<p>This model inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a>) – Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method to load the model
weights.</p>
</dd>
</dl>
<p>This class overrides <a class="reference internal" href="roberta.html#transformers.RobertaModelWithHeads" title="transformers.RobertaModelWithHeads"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaModelWithHeads</span></code></a>. Please check the superclass for the appropriate
documentation alongside usage examples.</p>
<dl class="py attribute">
<dt id="transformers.XLMRobertaModelWithHeads.config_class">
<code class="sig-name descname">config_class</code><a class="headerlink" href="#transformers.XLMRobertaModelWithHeads.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.xlm_roberta.configuration_xlm_roberta.XLMRobertaConfig</span></code></p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="xlmrobertaformaskedlm">
<h2>XLMRobertaForMaskedLM<a class="headerlink" href="#xlmrobertaformaskedlm" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.XLMRobertaForMaskedLM">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">XLMRobertaForMaskedLM</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.XLMRobertaForMaskedLM" title="Permalink to this definition">¶</a></dt>
<dd><p>XLM-RoBERTa Model with a <cite>language modeling</cite> head on top.</p>
<p>This model inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a>) – Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method to load the model
weights.</p>
</dd>
</dl>
<p>This class overrides <a class="reference internal" href="roberta.html#transformers.RobertaForMaskedLM" title="transformers.RobertaForMaskedLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMaskedLM</span></code></a>. Please check the superclass for the appropriate
documentation alongside usage examples.</p>
<dl class="py attribute">
<dt id="transformers.XLMRobertaForMaskedLM.config_class">
<code class="sig-name descname">config_class</code><a class="headerlink" href="#transformers.XLMRobertaForMaskedLM.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.xlm_roberta.configuration_xlm_roberta.XLMRobertaConfig</span></code></p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="xlmrobertaforsequenceclassification">
<h2>XLMRobertaForSequenceClassification<a class="headerlink" href="#xlmrobertaforsequenceclassification" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.XLMRobertaForSequenceClassification">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">XLMRobertaForSequenceClassification</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.XLMRobertaForSequenceClassification" title="Permalink to this definition">¶</a></dt>
<dd><p>XLM-RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.</p>
<p>This model inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a>) – Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method to load the model
weights.</p>
</dd>
</dl>
<p>This class overrides <a class="reference internal" href="roberta.html#transformers.RobertaForSequenceClassification" title="transformers.RobertaForSequenceClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForSequenceClassification</span></code></a>. Please check the superclass for the
appropriate documentation alongside usage examples.</p>
<dl class="py attribute">
<dt id="transformers.XLMRobertaForSequenceClassification.config_class">
<code class="sig-name descname">config_class</code><a class="headerlink" href="#transformers.XLMRobertaForSequenceClassification.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.xlm_roberta.configuration_xlm_roberta.XLMRobertaConfig</span></code></p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="xlmrobertaformultiplechoice">
<h2>XLMRobertaForMultipleChoice<a class="headerlink" href="#xlmrobertaformultiplechoice" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.XLMRobertaForMultipleChoice">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">XLMRobertaForMultipleChoice</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.XLMRobertaForMultipleChoice" title="Permalink to this definition">¶</a></dt>
<dd><p>XLM-RoBERTa Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.</p>
<p>This model inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a>) – Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method to load the model
weights.</p>
</dd>
</dl>
<p>This class overrides <code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForMultipleChoice</span></code>. Please check the superclass for the
appropriate documentation alongside usage examples.</p>
<dl class="py attribute">
<dt id="transformers.XLMRobertaForMultipleChoice.config_class">
<code class="sig-name descname">config_class</code><a class="headerlink" href="#transformers.XLMRobertaForMultipleChoice.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.xlm_roberta.configuration_xlm_roberta.XLMRobertaConfig</span></code></p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="xlmrobertafortokenclassification">
<h2>XLMRobertaForTokenClassification<a class="headerlink" href="#xlmrobertafortokenclassification" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="transformers.XLMRobertaForTokenClassification">
<em class="property">class </em><code class="sig-prename descclassname">transformers.</code><code class="sig-name descname">XLMRobertaForTokenClassification</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">config</span></em><span class="sig-paren">)</span><a class="headerlink" href="#transformers.XLMRobertaForTokenClassification" title="Permalink to this definition">¶</a></dt>
<dd><p>XLM-RoBERTa Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.</p>
<p>This model inherits from <code class="xref py py-class docutils literal notranslate"><span class="pre">PreTrainedModel</span></code>. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)</p>
<p>This model is also a PyTorch <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module">torch.nn.Module</a>
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<a class="reference internal" href="#transformers.XLMRobertaConfig" title="transformers.XLMRobertaConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></a>) – Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <code class="xref py py-meth docutils literal notranslate"><span class="pre">from_pretrained()</span></code> method to load the model
weights.</p>
</dd>
</dl>
<p>This class overrides <a class="reference internal" href="roberta.html#transformers.RobertaForTokenClassification" title="transformers.RobertaForTokenClassification"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForTokenClassification</span></code></a>. Please check the superclass for the
appropriate documentation alongside usage examples.</p>
<dl class="py attribute">
<dt id="transformers.XLMRobertaForTokenClassification.config_class">
<code class="sig-name descname">config_class</code><a class="headerlink" href="#transformers.XLMRobertaForTokenClassification.config_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">transformers.models.xlm_roberta.configuration_xlm_roberta.XLMRobertaConfig</span></code></p>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="roberta.html" class="btn btn-neutral float-left" title="RoBERTa" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Adapter-Hub Team

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>